{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wickedWOLF123/DRP/blob/main/wordModellingPytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Modelling Using LSTM**\n",
        "\n",
        "This is a implementation from https://www.tensorflow.org/text/tutorials/text_generation in TensorFlow based on the http://karpathy.github.io/2015/05/21/rnn-effectiveness/ by Andrej Karapathy. This is being implemented in Pytorch.\n",
        "\n",
        "We are building a Word Modelling ie next token prediction. We are trying this for generation of Shakespeare like text to understand the usefullness of LSTMS for Natural Language Processing Tasks"
      ],
      "metadata": {
        "id": "W1ZbpIfyhnzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports for this project\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "B4J42OChhzcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Shakespear writing as text file from googleapis\n",
        "import requests\n",
        "\n",
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "print(text[:250])\n",
        "\n",
        "# We need to convert every character to a vector so were see how many unique characters\n",
        "# These unique characters make up our vocabulary\n",
        "vocabulary = sorted(set(text))\n",
        "vocab_size = len(vocabulary)\n",
        "print(f'{len(vocabulary)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFZPnvRDiFPt",
        "outputId": "c924a5f4-9c77-41e7-bb82-a41daa526963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mappings from characters to vectors and vice-versa\n",
        "letter_to_index = {character: idx for idx, character in enumerate(vocabulary)}\n",
        "index_to_letter = {idx: character for idx, character in enumerate(vocabulary)}"
      ],
      "metadata": {
        "id": "0RFLC9rRn3xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Were going to chop up our 1000000+ character input into 100 size pieces\n",
        "# So that it is easier and we get batch processing\n",
        "sequence_length = 100\n",
        "encoded_text = np.array([letter_to_index[ch] for ch in text], dtype=np.int64)"
      ],
      "metadata": {
        "id": "rZwVjn73AfoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slice the encoded into sizes of encoded lenght\n",
        "# Our input sequence is from [0: seq_len] and the\n",
        "# the target sequence is [1: seq_len+1], now we loop\n",
        "input = []\n",
        "output = []\n",
        "\n",
        "for i in range(0, len(encoded_text) - sequence_length):\n",
        "  input.append(encoded_text[i:i+sequence_length])\n",
        "  output.append(encoded_text[i+1:i+sequence_length+1])\n",
        "\n",
        "# Converting to arrays for faster\n",
        "input = np.array(input)\n",
        "output = np.array(output)\n",
        "\n",
        "print(f'Sequences = {len(input)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GAHEw9kFEIw",
        "outputId": "2973519d-95c3-46fe-a4e7-732b8fbcff3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequences = 1115294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PARAMETERS\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "NUM_BATCHES = len(input) // BATCH_SIZE\n",
        "\n",
        "EMBED_SIZE = 128\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_EPOCHS = 10\n",
        "NUM_LAYERS = 2\n"
      ],
      "metadata": {
        "id": "msYqkXWubK8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aJCB4pNCAgXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Dataset in Pytorch and change them to tensors\n",
        "\n",
        "class shakespeareDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, input, output):\n",
        "    self.input = torch.tensor(input, dtype=torch.long)\n",
        "    self.output = torch.tensor(output, dtype=torch.long)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input[idx], self.output[idx]\n",
        "\n",
        "dataset = shakespeareDataset(input, output)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE , shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "KP-Kl2mla4OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model\n",
        "# We start with the embedding from the vector to an embedding\n",
        "# Then the torch.nn.lstm layer for the 2nd layer\n",
        "# And a fully connected dense layer that gives us the output as the\n",
        "# Same size as out input token.\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = out.contiguous().view(-1, out.shape[2])\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialize hidden state and cell state to zeros\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_(),\n",
        "                  weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_())\n",
        "        return hidden\n"
      ],
      "metadata": {
        "id": "9l_LPcXAGPyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMModel(vocab_size, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EsuOx1pN8kW",
        "outputId": "7ae15c05-561b-409e-b0ae-96f22db579b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (embedding): Embedding(65, 128)\n",
              "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n",
              "  (fc): Linear(in_features=256, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    hidden = model.init_hidden(BATCH_SIZE)\n",
        "    hidden = tuple([h.to(device) for h in hidden])\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for batch_idx, (x, y) in enumerate(dataloader):\n",
        "        if batch_idx >= NUM_BATCHES:\n",
        "            break\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # Detach hidden states to prevent backpropagating through the entire history\n",
        "        hidden = tuple([h.detach() for h in hidden])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output, hidden = model(x, hidden)\n",
        "        loss = criterion(output, y.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % 250 == 0:\n",
        "            print(f'Epoch [{epoch}/{NUM_EPOCHS}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f'Epoch [{epoch}/{NUM_EPOCHS}] completed with Average Loss: {avg_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqFu80P4ToNN",
        "outputId": "edbc1b74-69f9-40e9-d1d4-e102f89290d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Batch [250/17426], Loss: 1.8550\n",
            "Epoch [1/10], Batch [500/17426], Loss: 1.6276\n",
            "Epoch [1/10], Batch [750/17426], Loss: 1.4772\n",
            "Epoch [1/10], Batch [1000/17426], Loss: 1.4709\n",
            "Epoch [1/10], Batch [1250/17426], Loss: 1.4107\n",
            "Epoch [1/10], Batch [1500/17426], Loss: 1.3745\n",
            "Epoch [1/10], Batch [1750/17426], Loss: 1.3502\n",
            "Epoch [1/10], Batch [2000/17426], Loss: 1.3560\n",
            "Epoch [1/10], Batch [2250/17426], Loss: 1.3606\n",
            "Epoch [1/10], Batch [2500/17426], Loss: 1.3346\n",
            "Epoch [1/10], Batch [2750/17426], Loss: 1.2859\n",
            "Epoch [1/10], Batch [3000/17426], Loss: 1.3213\n",
            "Epoch [1/10], Batch [3250/17426], Loss: 1.2832\n",
            "Epoch [1/10], Batch [3500/17426], Loss: 1.2872\n",
            "Epoch [1/10], Batch [3750/17426], Loss: 1.2979\n",
            "Epoch [1/10], Batch [4000/17426], Loss: 1.2338\n",
            "Epoch [1/10], Batch [4250/17426], Loss: 1.2444\n",
            "Epoch [1/10], Batch [4500/17426], Loss: 1.2648\n",
            "Epoch [1/10], Batch [4750/17426], Loss: 1.2114\n",
            "Epoch [1/10], Batch [5000/17426], Loss: 1.2387\n",
            "Epoch [1/10], Batch [5250/17426], Loss: 1.1981\n",
            "Epoch [1/10], Batch [5500/17426], Loss: 1.1703\n",
            "Epoch [1/10], Batch [5750/17426], Loss: 1.2077\n",
            "Epoch [1/10], Batch [6000/17426], Loss: 1.1851\n",
            "Epoch [1/10], Batch [6250/17426], Loss: 1.1922\n",
            "Epoch [1/10], Batch [6500/17426], Loss: 1.1998\n",
            "Epoch [1/10], Batch [6750/17426], Loss: 1.2021\n",
            "Epoch [1/10], Batch [7000/17426], Loss: 1.1587\n",
            "Epoch [1/10], Batch [7250/17426], Loss: 1.1469\n",
            "Epoch [1/10], Batch [7500/17426], Loss: 1.1511\n",
            "Epoch [1/10], Batch [7750/17426], Loss: 1.1540\n",
            "Epoch [1/10], Batch [8000/17426], Loss: 1.1308\n",
            "Epoch [1/10], Batch [8250/17426], Loss: 1.1450\n",
            "Epoch [1/10], Batch [8500/17426], Loss: 1.1385\n",
            "Epoch [1/10], Batch [8750/17426], Loss: 1.1361\n",
            "Epoch [1/10], Batch [9000/17426], Loss: 1.0892\n",
            "Epoch [1/10], Batch [9250/17426], Loss: 1.1429\n",
            "Epoch [1/10], Batch [9500/17426], Loss: 1.1189\n",
            "Epoch [1/10], Batch [9750/17426], Loss: 1.0797\n",
            "Epoch [1/10], Batch [10000/17426], Loss: 1.0975\n",
            "Epoch [1/10], Batch [10250/17426], Loss: 1.0527\n",
            "Epoch [1/10], Batch [10500/17426], Loss: 1.1100\n",
            "Epoch [1/10], Batch [10750/17426], Loss: 1.1106\n",
            "Epoch [1/10], Batch [11000/17426], Loss: 1.1053\n",
            "Epoch [1/10], Batch [11250/17426], Loss: 1.0970\n",
            "Epoch [1/10], Batch [11500/17426], Loss: 1.0637\n",
            "Epoch [1/10], Batch [11750/17426], Loss: 1.0268\n",
            "Epoch [1/10], Batch [12000/17426], Loss: 1.0395\n",
            "Epoch [1/10], Batch [12250/17426], Loss: 1.0664\n",
            "Epoch [1/10], Batch [12500/17426], Loss: 1.0207\n",
            "Epoch [1/10], Batch [12750/17426], Loss: 1.0677\n",
            "Epoch [1/10], Batch [13000/17426], Loss: 1.0289\n",
            "Epoch [1/10], Batch [13250/17426], Loss: 1.0180\n",
            "Epoch [1/10], Batch [13500/17426], Loss: 1.0339\n",
            "Epoch [1/10], Batch [13750/17426], Loss: 1.0297\n",
            "Epoch [1/10], Batch [14000/17426], Loss: 1.0274\n",
            "Epoch [1/10], Batch [14250/17426], Loss: 0.9868\n",
            "Epoch [1/10], Batch [14500/17426], Loss: 1.0423\n",
            "Epoch [1/10], Batch [14750/17426], Loss: 1.0231\n",
            "Epoch [1/10], Batch [15000/17426], Loss: 1.0056\n",
            "Epoch [1/10], Batch [15250/17426], Loss: 0.9902\n",
            "Epoch [1/10], Batch [15500/17426], Loss: 0.9838\n",
            "Epoch [1/10], Batch [15750/17426], Loss: 0.9915\n",
            "Epoch [1/10], Batch [16000/17426], Loss: 0.9855\n",
            "Epoch [1/10], Batch [16250/17426], Loss: 0.9373\n",
            "Epoch [1/10], Batch [16500/17426], Loss: 0.9655\n",
            "Epoch [1/10], Batch [16750/17426], Loss: 0.9637\n",
            "Epoch [1/10], Batch [17000/17426], Loss: 0.9978\n",
            "Epoch [1/10], Batch [17250/17426], Loss: 0.9499\n",
            "Epoch [1/10] completed with Average Loss: 1.1723\n",
            "Epoch [2/10], Batch [250/17426], Loss: 0.9530\n",
            "Epoch [2/10], Batch [500/17426], Loss: 0.9575\n",
            "Epoch [2/10], Batch [750/17426], Loss: 0.9669\n",
            "Epoch [2/10], Batch [1000/17426], Loss: 0.9320\n",
            "Epoch [2/10], Batch [1250/17426], Loss: 0.9387\n",
            "Epoch [2/10], Batch [1500/17426], Loss: 0.9236\n",
            "Epoch [2/10], Batch [1750/17426], Loss: 0.9230\n",
            "Epoch [2/10], Batch [2000/17426], Loss: 0.9471\n",
            "Epoch [2/10], Batch [2250/17426], Loss: 0.9585\n",
            "Epoch [2/10], Batch [2500/17426], Loss: 0.9182\n",
            "Epoch [2/10], Batch [2750/17426], Loss: 0.9151\n",
            "Epoch [2/10], Batch [3000/17426], Loss: 0.9117\n",
            "Epoch [2/10], Batch [3250/17426], Loss: 0.8886\n",
            "Epoch [2/10], Batch [3500/17426], Loss: 0.9288\n",
            "Epoch [2/10], Batch [3750/17426], Loss: 0.9351\n",
            "Epoch [2/10], Batch [4000/17426], Loss: 0.9005\n",
            "Epoch [2/10], Batch [4250/17426], Loss: 0.9156\n",
            "Epoch [2/10], Batch [4500/17426], Loss: 0.9061\n",
            "Epoch [2/10], Batch [4750/17426], Loss: 0.9164\n",
            "Epoch [2/10], Batch [5000/17426], Loss: 0.8940\n",
            "Epoch [2/10], Batch [5250/17426], Loss: 0.9053\n",
            "Epoch [2/10], Batch [5500/17426], Loss: 0.8926\n",
            "Epoch [2/10], Batch [5750/17426], Loss: 0.9090\n",
            "Epoch [2/10], Batch [6000/17426], Loss: 0.9069\n",
            "Epoch [2/10], Batch [6250/17426], Loss: 0.8937\n",
            "Epoch [2/10], Batch [6500/17426], Loss: 0.8843\n",
            "Epoch [2/10], Batch [6750/17426], Loss: 0.9213\n",
            "Epoch [2/10], Batch [7000/17426], Loss: 0.8798\n",
            "Epoch [2/10], Batch [7250/17426], Loss: 0.9017\n",
            "Epoch [2/10], Batch [7500/17426], Loss: 0.8844\n",
            "Epoch [2/10], Batch [7750/17426], Loss: 0.8785\n",
            "Epoch [2/10], Batch [8000/17426], Loss: 0.8719\n",
            "Epoch [2/10], Batch [8250/17426], Loss: 0.9058\n",
            "Epoch [2/10], Batch [8500/17426], Loss: 0.8564\n",
            "Epoch [2/10], Batch [8750/17426], Loss: 0.8910\n",
            "Epoch [2/10], Batch [9000/17426], Loss: 0.8534\n",
            "Epoch [2/10], Batch [9250/17426], Loss: 0.8526\n",
            "Epoch [2/10], Batch [9500/17426], Loss: 0.8665\n",
            "Epoch [2/10], Batch [9750/17426], Loss: 0.8715\n",
            "Epoch [2/10], Batch [10000/17426], Loss: 0.8742\n",
            "Epoch [2/10], Batch [10250/17426], Loss: 0.8760\n",
            "Epoch [2/10], Batch [10500/17426], Loss: 0.8641\n",
            "Epoch [2/10], Batch [10750/17426], Loss: 0.8666\n",
            "Epoch [2/10], Batch [11000/17426], Loss: 0.8798\n",
            "Epoch [2/10], Batch [11250/17426], Loss: 0.8597\n",
            "Epoch [2/10], Batch [11500/17426], Loss: 0.8652\n",
            "Epoch [2/10], Batch [11750/17426], Loss: 0.8716\n",
            "Epoch [2/10], Batch [12000/17426], Loss: 0.8886\n",
            "Epoch [2/10], Batch [12250/17426], Loss: 0.8952\n",
            "Epoch [2/10], Batch [12500/17426], Loss: 0.8644\n",
            "Epoch [2/10], Batch [12750/17426], Loss: 0.8688\n",
            "Epoch [2/10], Batch [13000/17426], Loss: 0.8772\n",
            "Epoch [2/10], Batch [13250/17426], Loss: 0.8394\n",
            "Epoch [2/10], Batch [13500/17426], Loss: 0.8876\n",
            "Epoch [2/10], Batch [13750/17426], Loss: 0.8719\n",
            "Epoch [2/10], Batch [14000/17426], Loss: 0.8728\n",
            "Epoch [2/10], Batch [14250/17426], Loss: 0.8923\n",
            "Epoch [2/10], Batch [14500/17426], Loss: 0.8576\n",
            "Epoch [2/10], Batch [14750/17426], Loss: 0.8344\n",
            "Epoch [2/10], Batch [15000/17426], Loss: 0.8760\n",
            "Epoch [2/10], Batch [15250/17426], Loss: 0.8208\n",
            "Epoch [2/10], Batch [15500/17426], Loss: 0.8709\n",
            "Epoch [2/10], Batch [15750/17426], Loss: 0.8296\n",
            "Epoch [2/10], Batch [16000/17426], Loss: 0.8650\n",
            "Epoch [2/10], Batch [16250/17426], Loss: 0.8457\n",
            "Epoch [2/10], Batch [16500/17426], Loss: 0.8646\n",
            "Epoch [2/10], Batch [16750/17426], Loss: 0.8407\n",
            "Epoch [2/10], Batch [17000/17426], Loss: 0.8299\n",
            "Epoch [2/10], Batch [17250/17426], Loss: 0.8712\n",
            "Epoch [2/10] completed with Average Loss: 0.8872\n",
            "Epoch [3/10], Batch [250/17426], Loss: 0.8530\n",
            "Epoch [3/10], Batch [500/17426], Loss: 0.8178\n",
            "Epoch [3/10], Batch [750/17426], Loss: 0.8300\n",
            "Epoch [3/10], Batch [1000/17426], Loss: 0.8516\n",
            "Epoch [3/10], Batch [1250/17426], Loss: 0.8302\n",
            "Epoch [3/10], Batch [1500/17426], Loss: 0.8220\n",
            "Epoch [3/10], Batch [1750/17426], Loss: 0.8065\n",
            "Epoch [3/10], Batch [2000/17426], Loss: 0.8277\n",
            "Epoch [3/10], Batch [2250/17426], Loss: 0.8367\n",
            "Epoch [3/10], Batch [2500/17426], Loss: 0.8340\n",
            "Epoch [3/10], Batch [2750/17426], Loss: 0.8325\n",
            "Epoch [3/10], Batch [3000/17426], Loss: 0.8090\n",
            "Epoch [3/10], Batch [3250/17426], Loss: 0.8280\n",
            "Epoch [3/10], Batch [3500/17426], Loss: 0.8197\n",
            "Epoch [3/10], Batch [3750/17426], Loss: 0.8256\n",
            "Epoch [3/10], Batch [4000/17426], Loss: 0.8529\n",
            "Epoch [3/10], Batch [4250/17426], Loss: 0.8429\n",
            "Epoch [3/10], Batch [4500/17426], Loss: 0.8014\n",
            "Epoch [3/10], Batch [4750/17426], Loss: 0.8151\n",
            "Epoch [3/10], Batch [5000/17426], Loss: 0.8093\n",
            "Epoch [3/10], Batch [5250/17426], Loss: 0.8184\n",
            "Epoch [3/10], Batch [5500/17426], Loss: 0.8134\n",
            "Epoch [3/10], Batch [5750/17426], Loss: 0.8397\n",
            "Epoch [3/10], Batch [6000/17426], Loss: 0.8288\n",
            "Epoch [3/10], Batch [6250/17426], Loss: 0.8308\n",
            "Epoch [3/10], Batch [6500/17426], Loss: 0.7992\n",
            "Epoch [3/10], Batch [6750/17426], Loss: 0.8259\n",
            "Epoch [3/10], Batch [7000/17426], Loss: 0.8069\n",
            "Epoch [3/10], Batch [7250/17426], Loss: 0.8466\n",
            "Epoch [3/10], Batch [7500/17426], Loss: 0.7840\n",
            "Epoch [3/10], Batch [7750/17426], Loss: 0.8245\n",
            "Epoch [3/10], Batch [8000/17426], Loss: 0.8018\n",
            "Epoch [3/10], Batch [8250/17426], Loss: 0.7753\n",
            "Epoch [3/10], Batch [8500/17426], Loss: 0.8452\n",
            "Epoch [3/10], Batch [8750/17426], Loss: 0.7908\n",
            "Epoch [3/10], Batch [9000/17426], Loss: 0.7910\n",
            "Epoch [3/10], Batch [9250/17426], Loss: 0.8050\n",
            "Epoch [3/10], Batch [9500/17426], Loss: 0.8320\n",
            "Epoch [3/10], Batch [9750/17426], Loss: 0.8194\n",
            "Epoch [3/10], Batch [10000/17426], Loss: 0.8017\n",
            "Epoch [3/10], Batch [10250/17426], Loss: 0.8036\n",
            "Epoch [3/10], Batch [10500/17426], Loss: 0.8108\n",
            "Epoch [3/10], Batch [10750/17426], Loss: 0.8385\n",
            "Epoch [3/10], Batch [11000/17426], Loss: 0.7987\n",
            "Epoch [3/10], Batch [11250/17426], Loss: 0.8128\n",
            "Epoch [3/10], Batch [11500/17426], Loss: 0.7932\n",
            "Epoch [3/10], Batch [11750/17426], Loss: 0.7907\n",
            "Epoch [3/10], Batch [12000/17426], Loss: 0.8269\n",
            "Epoch [3/10], Batch [12250/17426], Loss: 0.7937\n",
            "Epoch [3/10], Batch [12500/17426], Loss: 0.8016\n",
            "Epoch [3/10], Batch [12750/17426], Loss: 0.8303\n",
            "Epoch [3/10], Batch [13000/17426], Loss: 0.8162\n",
            "Epoch [3/10], Batch [13250/17426], Loss: 0.7794\n",
            "Epoch [3/10], Batch [13500/17426], Loss: 0.7656\n",
            "Epoch [3/10], Batch [13750/17426], Loss: 0.8132\n",
            "Epoch [3/10], Batch [14000/17426], Loss: 0.8334\n",
            "Epoch [3/10], Batch [14250/17426], Loss: 0.8252\n",
            "Epoch [3/10], Batch [14500/17426], Loss: 0.7814\n",
            "Epoch [3/10], Batch [14750/17426], Loss: 0.8058\n",
            "Epoch [3/10], Batch [15000/17426], Loss: 0.8141\n",
            "Epoch [3/10], Batch [15250/17426], Loss: 0.7994\n",
            "Epoch [3/10], Batch [15500/17426], Loss: 0.7842\n",
            "Epoch [3/10], Batch [15750/17426], Loss: 0.8057\n",
            "Epoch [3/10], Batch [16000/17426], Loss: 0.8231\n",
            "Epoch [3/10], Batch [16250/17426], Loss: 0.7621\n",
            "Epoch [3/10], Batch [16500/17426], Loss: 0.7981\n",
            "Epoch [3/10], Batch [16750/17426], Loss: 0.7910\n",
            "Epoch [3/10], Batch [17000/17426], Loss: 0.8045\n",
            "Epoch [3/10], Batch [17250/17426], Loss: 0.7751\n",
            "Epoch [3/10] completed with Average Loss: 0.8112\n",
            "Epoch [4/10], Batch [250/17426], Loss: 0.8008\n",
            "Epoch [4/10], Batch [500/17426], Loss: 0.7718\n",
            "Epoch [4/10], Batch [750/17426], Loss: 0.7933\n",
            "Epoch [4/10], Batch [1000/17426], Loss: 0.7974\n",
            "Epoch [4/10], Batch [1250/17426], Loss: 0.7701\n",
            "Epoch [4/10], Batch [1500/17426], Loss: 0.7679\n",
            "Epoch [4/10], Batch [1750/17426], Loss: 0.8292\n",
            "Epoch [4/10], Batch [2000/17426], Loss: 0.7883\n",
            "Epoch [4/10], Batch [2250/17426], Loss: 0.7888\n",
            "Epoch [4/10], Batch [2500/17426], Loss: 0.7669\n",
            "Epoch [4/10], Batch [2750/17426], Loss: 0.7767\n",
            "Epoch [4/10], Batch [3000/17426], Loss: 0.7607\n",
            "Epoch [4/10], Batch [3250/17426], Loss: 0.7896\n",
            "Epoch [4/10], Batch [3500/17426], Loss: 0.7936\n",
            "Epoch [4/10], Batch [3750/17426], Loss: 0.7807\n",
            "Epoch [4/10], Batch [4000/17426], Loss: 0.7670\n",
            "Epoch [4/10], Batch [4250/17426], Loss: 0.7767\n",
            "Epoch [4/10], Batch [4500/17426], Loss: 0.7622\n",
            "Epoch [4/10], Batch [4750/17426], Loss: 0.7798\n",
            "Epoch [4/10], Batch [5000/17426], Loss: 0.7706\n",
            "Epoch [4/10], Batch [5250/17426], Loss: 0.7591\n",
            "Epoch [4/10], Batch [5500/17426], Loss: 0.8020\n",
            "Epoch [4/10], Batch [5750/17426], Loss: 0.7913\n",
            "Epoch [4/10], Batch [6000/17426], Loss: 0.7895\n",
            "Epoch [4/10], Batch [6250/17426], Loss: 0.7958\n",
            "Epoch [4/10], Batch [6500/17426], Loss: 0.7668\n",
            "Epoch [4/10], Batch [6750/17426], Loss: 0.7897\n",
            "Epoch [4/10], Batch [7000/17426], Loss: 0.7873\n",
            "Epoch [4/10], Batch [7250/17426], Loss: 0.7913\n",
            "Epoch [4/10], Batch [7500/17426], Loss: 0.7466\n",
            "Epoch [4/10], Batch [7750/17426], Loss: 0.7990\n",
            "Epoch [4/10], Batch [8000/17426], Loss: 0.8026\n",
            "Epoch [4/10], Batch [8250/17426], Loss: 0.7624\n",
            "Epoch [4/10], Batch [8500/17426], Loss: 0.7665\n",
            "Epoch [4/10], Batch [8750/17426], Loss: 0.7614\n",
            "Epoch [4/10], Batch [9000/17426], Loss: 0.7938\n",
            "Epoch [4/10], Batch [9250/17426], Loss: 0.7802\n",
            "Epoch [4/10], Batch [9500/17426], Loss: 0.7864\n",
            "Epoch [4/10], Batch [9750/17426], Loss: 0.7703\n",
            "Epoch [4/10], Batch [10000/17426], Loss: 0.7687\n",
            "Epoch [4/10], Batch [10250/17426], Loss: 0.7594\n",
            "Epoch [4/10], Batch [10500/17426], Loss: 0.7403\n",
            "Epoch [4/10], Batch [10750/17426], Loss: 0.7917\n",
            "Epoch [4/10], Batch [11000/17426], Loss: 0.7828\n",
            "Epoch [4/10], Batch [11250/17426], Loss: 0.7737\n",
            "Epoch [4/10], Batch [11500/17426], Loss: 0.7599\n",
            "Epoch [4/10], Batch [11750/17426], Loss: 0.7656\n",
            "Epoch [4/10], Batch [12000/17426], Loss: 0.7683\n",
            "Epoch [4/10], Batch [12250/17426], Loss: 0.7524\n",
            "Epoch [4/10], Batch [12500/17426], Loss: 0.7871\n",
            "Epoch [4/10], Batch [12750/17426], Loss: 0.7685\n",
            "Epoch [4/10], Batch [13000/17426], Loss: 0.7951\n",
            "Epoch [4/10], Batch [13250/17426], Loss: 0.7703\n",
            "Epoch [4/10], Batch [13500/17426], Loss: 0.7613\n",
            "Epoch [4/10], Batch [13750/17426], Loss: 0.7450\n",
            "Epoch [4/10], Batch [14000/17426], Loss: 0.7794\n",
            "Epoch [4/10], Batch [14250/17426], Loss: 0.7931\n",
            "Epoch [4/10], Batch [14500/17426], Loss: 0.7630\n",
            "Epoch [4/10], Batch [14750/17426], Loss: 0.7731\n",
            "Epoch [4/10], Batch [15000/17426], Loss: 0.7889\n",
            "Epoch [4/10], Batch [15250/17426], Loss: 0.7631\n",
            "Epoch [4/10], Batch [15500/17426], Loss: 0.7799\n",
            "Epoch [4/10], Batch [15750/17426], Loss: 0.7943\n",
            "Epoch [4/10], Batch [16000/17426], Loss: 0.7761\n",
            "Epoch [4/10], Batch [16250/17426], Loss: 0.7681\n",
            "Epoch [4/10], Batch [16500/17426], Loss: 0.7685\n",
            "Epoch [4/10], Batch [16750/17426], Loss: 0.7934\n",
            "Epoch [4/10], Batch [17000/17426], Loss: 0.7715\n",
            "Epoch [4/10], Batch [17250/17426], Loss: 0.7467\n",
            "Epoch [4/10] completed with Average Loss: 0.7794\n",
            "Epoch [5/10], Batch [250/17426], Loss: 0.7505\n",
            "Epoch [5/10], Batch [500/17426], Loss: 0.7895\n",
            "Epoch [5/10], Batch [750/17426], Loss: 0.7587\n",
            "Epoch [5/10], Batch [1000/17426], Loss: 0.7833\n",
            "Epoch [5/10], Batch [1250/17426], Loss: 0.7784\n",
            "Epoch [5/10], Batch [1500/17426], Loss: 0.7783\n",
            "Epoch [5/10], Batch [1750/17426], Loss: 0.7635\n",
            "Epoch [5/10], Batch [2000/17426], Loss: 0.7859\n",
            "Epoch [5/10], Batch [2250/17426], Loss: 0.7598\n",
            "Epoch [5/10], Batch [2500/17426], Loss: 0.7676\n",
            "Epoch [5/10], Batch [2750/17426], Loss: 0.7302\n",
            "Epoch [5/10], Batch [3000/17426], Loss: 0.7368\n",
            "Epoch [5/10], Batch [3250/17426], Loss: 0.7617\n",
            "Epoch [5/10], Batch [3500/17426], Loss: 0.7868\n",
            "Epoch [5/10], Batch [3750/17426], Loss: 0.7627\n",
            "Epoch [5/10], Batch [4000/17426], Loss: 0.7712\n",
            "Epoch [5/10], Batch [4250/17426], Loss: 0.7441\n",
            "Epoch [5/10], Batch [4500/17426], Loss: 0.7397\n",
            "Epoch [5/10], Batch [4750/17426], Loss: 0.7648\n",
            "Epoch [5/10], Batch [5000/17426], Loss: 0.7814\n",
            "Epoch [5/10], Batch [5250/17426], Loss: 0.7516\n",
            "Epoch [5/10], Batch [5500/17426], Loss: 0.7594\n",
            "Epoch [5/10], Batch [5750/17426], Loss: 0.7400\n",
            "Epoch [5/10], Batch [6000/17426], Loss: 0.7544\n",
            "Epoch [5/10], Batch [6250/17426], Loss: 0.7615\n",
            "Epoch [5/10], Batch [6500/17426], Loss: 0.7859\n",
            "Epoch [5/10], Batch [6750/17426], Loss: 0.7633\n",
            "Epoch [5/10], Batch [7000/17426], Loss: 0.7868\n",
            "Epoch [5/10], Batch [7250/17426], Loss: 0.7564\n",
            "Epoch [5/10], Batch [7500/17426], Loss: 0.7607\n",
            "Epoch [5/10], Batch [7750/17426], Loss: 0.7643\n",
            "Epoch [5/10], Batch [8000/17426], Loss: 0.7430\n",
            "Epoch [5/10], Batch [8250/17426], Loss: 0.7515\n",
            "Epoch [5/10], Batch [8500/17426], Loss: 0.7621\n",
            "Epoch [5/10], Batch [8750/17426], Loss: 0.7973\n",
            "Epoch [5/10], Batch [9000/17426], Loss: 0.7563\n",
            "Epoch [5/10], Batch [9250/17426], Loss: 0.7546\n",
            "Epoch [5/10], Batch [9500/17426], Loss: 0.7565\n",
            "Epoch [5/10], Batch [9750/17426], Loss: 0.7505\n",
            "Epoch [5/10], Batch [10000/17426], Loss: 0.7410\n",
            "Epoch [5/10], Batch [10250/17426], Loss: 0.7850\n",
            "Epoch [5/10], Batch [10500/17426], Loss: 0.7642\n",
            "Epoch [5/10], Batch [10750/17426], Loss: 0.7504\n",
            "Epoch [5/10], Batch [11000/17426], Loss: 0.7564\n",
            "Epoch [5/10], Batch [11250/17426], Loss: 0.7391\n",
            "Epoch [5/10], Batch [11500/17426], Loss: 0.7616\n",
            "Epoch [5/10], Batch [11750/17426], Loss: 0.7920\n",
            "Epoch [5/10], Batch [12000/17426], Loss: 0.7728\n",
            "Epoch [5/10], Batch [12250/17426], Loss: 0.8004\n",
            "Epoch [5/10], Batch [12500/17426], Loss: 0.7746\n",
            "Epoch [5/10], Batch [12750/17426], Loss: 0.7489\n",
            "Epoch [5/10], Batch [13000/17426], Loss: 0.7617\n",
            "Epoch [5/10], Batch [13250/17426], Loss: 0.7672\n",
            "Epoch [5/10], Batch [13500/17426], Loss: 0.7809\n",
            "Epoch [5/10], Batch [13750/17426], Loss: 0.7624\n",
            "Epoch [5/10], Batch [14000/17426], Loss: 0.7638\n",
            "Epoch [5/10], Batch [14250/17426], Loss: 0.7625\n",
            "Epoch [5/10], Batch [14500/17426], Loss: 0.7500\n",
            "Epoch [5/10], Batch [14750/17426], Loss: 0.7663\n",
            "Epoch [5/10], Batch [15000/17426], Loss: 0.7695\n",
            "Epoch [5/10], Batch [15250/17426], Loss: 0.7702\n",
            "Epoch [5/10], Batch [15500/17426], Loss: 0.7507\n",
            "Epoch [5/10], Batch [15750/17426], Loss: 0.7550\n",
            "Epoch [5/10], Batch [16000/17426], Loss: 0.7424\n",
            "Epoch [5/10], Batch [16250/17426], Loss: 0.7205\n",
            "Epoch [5/10], Batch [16500/17426], Loss: 0.7517\n",
            "Epoch [5/10], Batch [16750/17426], Loss: 0.7544\n",
            "Epoch [5/10], Batch [17000/17426], Loss: 0.7209\n",
            "Epoch [5/10], Batch [17250/17426], Loss: 0.7727\n",
            "Epoch [5/10] completed with Average Loss: 0.7606\n",
            "Epoch [6/10], Batch [250/17426], Loss: 0.7369\n",
            "Epoch [6/10], Batch [500/17426], Loss: 0.7311\n",
            "Epoch [6/10], Batch [750/17426], Loss: 0.7763\n",
            "Epoch [6/10], Batch [1000/17426], Loss: 0.7403\n",
            "Epoch [6/10], Batch [1250/17426], Loss: 0.7252\n",
            "Epoch [6/10], Batch [1500/17426], Loss: 0.7498\n",
            "Epoch [6/10], Batch [1750/17426], Loss: 0.7840\n",
            "Epoch [6/10], Batch [2000/17426], Loss: 0.7472\n",
            "Epoch [6/10], Batch [2250/17426], Loss: 0.7184\n",
            "Epoch [6/10], Batch [2500/17426], Loss: 0.7529\n",
            "Epoch [6/10], Batch [2750/17426], Loss: 0.7564\n",
            "Epoch [6/10], Batch [3000/17426], Loss: 0.7332\n",
            "Epoch [6/10], Batch [3250/17426], Loss: 0.7306\n",
            "Epoch [6/10], Batch [3500/17426], Loss: 0.7417\n",
            "Epoch [6/10], Batch [3750/17426], Loss: 0.7381\n",
            "Epoch [6/10], Batch [4000/17426], Loss: 0.7647\n",
            "Epoch [6/10], Batch [4250/17426], Loss: 0.7474\n",
            "Epoch [6/10], Batch [4500/17426], Loss: 0.7339\n",
            "Epoch [6/10], Batch [4750/17426], Loss: 0.7594\n",
            "Epoch [6/10], Batch [5000/17426], Loss: 0.7496\n",
            "Epoch [6/10], Batch [5250/17426], Loss: 0.7267\n",
            "Epoch [6/10], Batch [5500/17426], Loss: 0.7771\n",
            "Epoch [6/10], Batch [5750/17426], Loss: 0.7601\n",
            "Epoch [6/10], Batch [6000/17426], Loss: 0.7590\n",
            "Epoch [6/10], Batch [6250/17426], Loss: 0.7501\n",
            "Epoch [6/10], Batch [6500/17426], Loss: 0.7437\n",
            "Epoch [6/10], Batch [6750/17426], Loss: 0.7802\n",
            "Epoch [6/10], Batch [7000/17426], Loss: 0.7585\n",
            "Epoch [6/10], Batch [7250/17426], Loss: 0.7530\n",
            "Epoch [6/10], Batch [7500/17426], Loss: 0.7364\n",
            "Epoch [6/10], Batch [7750/17426], Loss: 0.7380\n",
            "Epoch [6/10], Batch [8000/17426], Loss: 0.7610\n",
            "Epoch [6/10], Batch [8250/17426], Loss: 0.7388\n",
            "Epoch [6/10], Batch [8500/17426], Loss: 0.7290\n",
            "Epoch [6/10], Batch [8750/17426], Loss: 0.7250\n",
            "Epoch [6/10], Batch [9000/17426], Loss: 0.7521\n",
            "Epoch [6/10], Batch [9250/17426], Loss: 0.7608\n",
            "Epoch [6/10], Batch [9500/17426], Loss: 0.7520\n",
            "Epoch [6/10], Batch [9750/17426], Loss: 0.7484\n",
            "Epoch [6/10], Batch [10000/17426], Loss: 0.7309\n",
            "Epoch [6/10], Batch [10250/17426], Loss: 0.7401\n",
            "Epoch [6/10], Batch [10500/17426], Loss: 0.7622\n",
            "Epoch [6/10], Batch [10750/17426], Loss: 0.7441\n",
            "Epoch [6/10], Batch [11000/17426], Loss: 0.7291\n",
            "Epoch [6/10], Batch [11250/17426], Loss: 0.7220\n",
            "Epoch [6/10], Batch [11500/17426], Loss: 0.7774\n",
            "Epoch [6/10], Batch [11750/17426], Loss: 0.7462\n",
            "Epoch [6/10], Batch [12000/17426], Loss: 0.7340\n",
            "Epoch [6/10], Batch [12250/17426], Loss: 0.7229\n",
            "Epoch [6/10], Batch [12500/17426], Loss: 0.7519\n",
            "Epoch [6/10], Batch [12750/17426], Loss: 0.7629\n",
            "Epoch [6/10], Batch [13000/17426], Loss: 0.7291\n",
            "Epoch [6/10], Batch [13250/17426], Loss: 0.7072\n",
            "Epoch [6/10], Batch [13500/17426], Loss: 0.7094\n",
            "Epoch [6/10], Batch [13750/17426], Loss: 0.7347\n",
            "Epoch [6/10], Batch [14000/17426], Loss: 0.7312\n",
            "Epoch [6/10], Batch [14250/17426], Loss: 0.7306\n",
            "Epoch [6/10], Batch [14500/17426], Loss: 0.7303\n",
            "Epoch [6/10], Batch [14750/17426], Loss: 0.7540\n",
            "Epoch [6/10], Batch [15000/17426], Loss: 0.7128\n",
            "Epoch [6/10], Batch [15250/17426], Loss: 0.7511\n",
            "Epoch [6/10], Batch [15500/17426], Loss: 0.7426\n",
            "Epoch [6/10], Batch [15750/17426], Loss: 0.7470\n",
            "Epoch [6/10], Batch [16000/17426], Loss: 0.7449\n",
            "Epoch [6/10], Batch [16250/17426], Loss: 0.7376\n",
            "Epoch [6/10], Batch [16500/17426], Loss: 0.7412\n",
            "Epoch [6/10], Batch [16750/17426], Loss: 0.7473\n",
            "Epoch [6/10], Batch [17000/17426], Loss: 0.7132\n",
            "Epoch [6/10], Batch [17250/17426], Loss: 0.7646\n",
            "Epoch [6/10] completed with Average Loss: 0.7479\n",
            "Epoch [7/10], Batch [250/17426], Loss: 0.7172\n",
            "Epoch [7/10], Batch [500/17426], Loss: 0.7874\n",
            "Epoch [7/10], Batch [750/17426], Loss: 0.7641\n",
            "Epoch [7/10], Batch [1000/17426], Loss: 0.7403\n",
            "Epoch [7/10], Batch [1250/17426], Loss: 0.7359\n",
            "Epoch [7/10], Batch [1500/17426], Loss: 0.7377\n",
            "Epoch [7/10], Batch [1750/17426], Loss: 0.7561\n",
            "Epoch [7/10], Batch [2000/17426], Loss: 0.7661\n",
            "Epoch [7/10], Batch [2250/17426], Loss: 0.7420\n",
            "Epoch [7/10], Batch [2500/17426], Loss: 0.7506\n",
            "Epoch [7/10], Batch [2750/17426], Loss: 0.7705\n",
            "Epoch [7/10], Batch [3000/17426], Loss: 0.7564\n",
            "Epoch [7/10], Batch [3250/17426], Loss: 0.7516\n",
            "Epoch [7/10], Batch [3500/17426], Loss: 0.7513\n",
            "Epoch [7/10], Batch [3750/17426], Loss: 0.7576\n",
            "Epoch [7/10], Batch [4000/17426], Loss: 0.7396\n",
            "Epoch [7/10], Batch [4250/17426], Loss: 0.7563\n",
            "Epoch [7/10], Batch [4500/17426], Loss: 0.7402\n",
            "Epoch [7/10], Batch [4750/17426], Loss: 0.7780\n",
            "Epoch [7/10], Batch [5000/17426], Loss: 0.7534\n",
            "Epoch [7/10], Batch [5250/17426], Loss: 0.7393\n",
            "Epoch [7/10], Batch [5500/17426], Loss: 0.7755\n",
            "Epoch [7/10], Batch [5750/17426], Loss: 0.7395\n",
            "Epoch [7/10], Batch [6000/17426], Loss: 0.7503\n",
            "Epoch [7/10], Batch [6250/17426], Loss: 0.7510\n",
            "Epoch [7/10], Batch [6500/17426], Loss: 0.7694\n",
            "Epoch [7/10], Batch [6750/17426], Loss: 0.7580\n",
            "Epoch [7/10], Batch [7000/17426], Loss: 0.7111\n",
            "Epoch [7/10], Batch [7250/17426], Loss: 0.7609\n",
            "Epoch [7/10], Batch [7500/17426], Loss: 0.7317\n",
            "Epoch [7/10], Batch [7750/17426], Loss: 0.7706\n",
            "Epoch [7/10], Batch [8000/17426], Loss: 0.7562\n",
            "Epoch [7/10], Batch [8250/17426], Loss: 0.7558\n",
            "Epoch [7/10], Batch [8500/17426], Loss: 0.7488\n",
            "Epoch [7/10], Batch [8750/17426], Loss: 0.7430\n",
            "Epoch [7/10], Batch [9000/17426], Loss: 0.7302\n",
            "Epoch [7/10], Batch [9250/17426], Loss: 0.7006\n",
            "Epoch [7/10], Batch [9500/17426], Loss: 0.7564\n",
            "Epoch [7/10], Batch [9750/17426], Loss: 0.7364\n",
            "Epoch [7/10], Batch [10000/17426], Loss: 0.7323\n",
            "Epoch [7/10], Batch [10250/17426], Loss: 0.7495\n",
            "Epoch [7/10], Batch [10500/17426], Loss: 0.7513\n",
            "Epoch [7/10], Batch [10750/17426], Loss: 0.7599\n",
            "Epoch [7/10], Batch [11000/17426], Loss: 0.7258\n",
            "Epoch [7/10], Batch [11250/17426], Loss: 0.7126\n",
            "Epoch [7/10], Batch [11500/17426], Loss: 0.7303\n",
            "Epoch [7/10], Batch [11750/17426], Loss: 0.6924\n",
            "Epoch [7/10], Batch [12000/17426], Loss: 0.7377\n",
            "Epoch [7/10], Batch [12250/17426], Loss: 0.7182\n",
            "Epoch [7/10], Batch [12500/17426], Loss: 0.7091\n",
            "Epoch [7/10], Batch [12750/17426], Loss: 0.7450\n",
            "Epoch [7/10], Batch [13000/17426], Loss: 0.7359\n",
            "Epoch [7/10], Batch [13250/17426], Loss: 0.7305\n",
            "Epoch [7/10], Batch [13500/17426], Loss: 0.7263\n",
            "Epoch [7/10], Batch [13750/17426], Loss: 0.7819\n",
            "Epoch [7/10], Batch [14000/17426], Loss: 0.7282\n",
            "Epoch [7/10], Batch [14250/17426], Loss: 0.7386\n",
            "Epoch [7/10], Batch [14500/17426], Loss: 0.7398\n",
            "Epoch [7/10], Batch [14750/17426], Loss: 0.7289\n",
            "Epoch [7/10], Batch [15000/17426], Loss: 0.7124\n",
            "Epoch [7/10], Batch [15250/17426], Loss: 0.7154\n",
            "Epoch [7/10], Batch [15500/17426], Loss: 0.7277\n",
            "Epoch [7/10], Batch [15750/17426], Loss: 0.7448\n",
            "Epoch [7/10], Batch [16000/17426], Loss: 0.7150\n",
            "Epoch [7/10], Batch [16250/17426], Loss: 0.7379\n",
            "Epoch [7/10], Batch [16500/17426], Loss: 0.7545\n",
            "Epoch [7/10], Batch [16750/17426], Loss: 0.7554\n",
            "Epoch [7/10], Batch [17000/17426], Loss: 0.7305\n",
            "Epoch [7/10], Batch [17250/17426], Loss: 0.7385\n",
            "Epoch [7/10] completed with Average Loss: 0.7382\n",
            "Epoch [8/10], Batch [250/17426], Loss: 0.7676\n",
            "Epoch [8/10], Batch [500/17426], Loss: 0.7394\n",
            "Epoch [8/10], Batch [750/17426], Loss: 0.7409\n",
            "Epoch [8/10], Batch [1000/17426], Loss: 0.7608\n",
            "Epoch [8/10], Batch [1250/17426], Loss: 0.7381\n",
            "Epoch [8/10], Batch [1500/17426], Loss: 0.7466\n",
            "Epoch [8/10], Batch [1750/17426], Loss: 0.6833\n",
            "Epoch [8/10], Batch [2000/17426], Loss: 0.7433\n",
            "Epoch [8/10], Batch [2250/17426], Loss: 0.7356\n",
            "Epoch [8/10], Batch [2500/17426], Loss: 0.7239\n",
            "Epoch [8/10], Batch [2750/17426], Loss: 0.7426\n",
            "Epoch [8/10], Batch [3000/17426], Loss: 0.7015\n",
            "Epoch [8/10], Batch [3250/17426], Loss: 0.7125\n",
            "Epoch [8/10], Batch [3500/17426], Loss: 0.7464\n",
            "Epoch [8/10], Batch [3750/17426], Loss: 0.7379\n",
            "Epoch [8/10], Batch [4000/17426], Loss: 0.7479\n",
            "Epoch [8/10], Batch [4250/17426], Loss: 0.7206\n",
            "Epoch [8/10], Batch [4500/17426], Loss: 0.7297\n",
            "Epoch [8/10], Batch [4750/17426], Loss: 0.7237\n",
            "Epoch [8/10], Batch [5000/17426], Loss: 0.7223\n",
            "Epoch [8/10], Batch [5250/17426], Loss: 0.7239\n",
            "Epoch [8/10], Batch [5500/17426], Loss: 0.7273\n",
            "Epoch [8/10], Batch [5750/17426], Loss: 0.7179\n",
            "Epoch [8/10], Batch [6000/17426], Loss: 0.7269\n",
            "Epoch [8/10], Batch [6250/17426], Loss: 0.7383\n",
            "Epoch [8/10], Batch [6500/17426], Loss: 0.7247\n",
            "Epoch [8/10], Batch [6750/17426], Loss: 0.7323\n",
            "Epoch [8/10], Batch [7000/17426], Loss: 0.7423\n",
            "Epoch [8/10], Batch [7250/17426], Loss: 0.7436\n",
            "Epoch [8/10], Batch [7500/17426], Loss: 0.7160\n",
            "Epoch [8/10], Batch [7750/17426], Loss: 0.7005\n",
            "Epoch [8/10], Batch [8000/17426], Loss: 0.7211\n",
            "Epoch [8/10], Batch [8250/17426], Loss: 0.7382\n",
            "Epoch [8/10], Batch [8500/17426], Loss: 0.7458\n",
            "Epoch [8/10], Batch [8750/17426], Loss: 0.7155\n",
            "Epoch [8/10], Batch [9000/17426], Loss: 0.7241\n",
            "Epoch [8/10], Batch [9250/17426], Loss: 0.7374\n",
            "Epoch [8/10], Batch [9500/17426], Loss: 0.7281\n",
            "Epoch [8/10], Batch [9750/17426], Loss: 0.7254\n",
            "Epoch [8/10], Batch [10000/17426], Loss: 0.7215\n",
            "Epoch [8/10], Batch [10250/17426], Loss: 0.7170\n",
            "Epoch [8/10], Batch [10500/17426], Loss: 0.7223\n",
            "Epoch [8/10], Batch [10750/17426], Loss: 0.7476\n",
            "Epoch [8/10], Batch [11000/17426], Loss: 0.7513\n",
            "Epoch [8/10], Batch [11250/17426], Loss: 0.7323\n",
            "Epoch [8/10], Batch [11500/17426], Loss: 0.7250\n",
            "Epoch [8/10], Batch [11750/17426], Loss: 0.7368\n",
            "Epoch [8/10], Batch [12000/17426], Loss: 0.7318\n",
            "Epoch [8/10], Batch [12250/17426], Loss: 0.7664\n",
            "Epoch [8/10], Batch [12500/17426], Loss: 0.7268\n",
            "Epoch [8/10], Batch [12750/17426], Loss: 0.7222\n",
            "Epoch [8/10], Batch [13000/17426], Loss: 0.7433\n",
            "Epoch [8/10], Batch [13250/17426], Loss: 0.7349\n",
            "Epoch [8/10], Batch [13500/17426], Loss: 0.7163\n",
            "Epoch [8/10], Batch [13750/17426], Loss: 0.7285\n",
            "Epoch [8/10], Batch [14000/17426], Loss: 0.7205\n",
            "Epoch [8/10], Batch [14250/17426], Loss: 0.7601\n",
            "Epoch [8/10], Batch [14500/17426], Loss: 0.7522\n",
            "Epoch [8/10], Batch [14750/17426], Loss: 0.7423\n",
            "Epoch [8/10], Batch [15000/17426], Loss: 0.7398\n",
            "Epoch [8/10], Batch [15250/17426], Loss: 0.7303\n",
            "Epoch [8/10], Batch [15500/17426], Loss: 0.6887\n",
            "Epoch [8/10], Batch [15750/17426], Loss: 0.7386\n",
            "Epoch [8/10], Batch [16000/17426], Loss: 0.7305\n",
            "Epoch [8/10], Batch [16250/17426], Loss: 0.7161\n",
            "Epoch [8/10], Batch [16500/17426], Loss: 0.7268\n",
            "Epoch [8/10], Batch [16750/17426], Loss: 0.7193\n",
            "Epoch [8/10], Batch [17000/17426], Loss: 0.7018\n",
            "Epoch [8/10], Batch [17250/17426], Loss: 0.7372\n",
            "Epoch [8/10] completed with Average Loss: 0.7308\n",
            "Epoch [9/10], Batch [250/17426], Loss: 0.7366\n",
            "Epoch [9/10], Batch [500/17426], Loss: 0.7318\n",
            "Epoch [9/10], Batch [750/17426], Loss: 0.7112\n",
            "Epoch [9/10], Batch [1000/17426], Loss: 0.7184\n",
            "Epoch [9/10], Batch [1250/17426], Loss: 0.6820\n",
            "Epoch [9/10], Batch [1500/17426], Loss: 0.7380\n",
            "Epoch [9/10], Batch [1750/17426], Loss: 0.7299\n",
            "Epoch [9/10], Batch [2000/17426], Loss: 0.7362\n",
            "Epoch [9/10], Batch [2250/17426], Loss: 0.6887\n",
            "Epoch [9/10], Batch [2500/17426], Loss: 0.7278\n",
            "Epoch [9/10], Batch [2750/17426], Loss: 0.7053\n",
            "Epoch [9/10], Batch [3000/17426], Loss: 0.7358\n",
            "Epoch [9/10], Batch [3250/17426], Loss: 0.6846\n",
            "Epoch [9/10], Batch [3500/17426], Loss: 0.7251\n",
            "Epoch [9/10], Batch [3750/17426], Loss: 0.7266\n",
            "Epoch [9/10], Batch [4000/17426], Loss: 0.7174\n",
            "Epoch [9/10], Batch [4250/17426], Loss: 0.7395\n",
            "Epoch [9/10], Batch [4500/17426], Loss: 0.7183\n",
            "Epoch [9/10], Batch [4750/17426], Loss: 0.7662\n",
            "Epoch [9/10], Batch [5000/17426], Loss: 0.7328\n",
            "Epoch [9/10], Batch [5250/17426], Loss: 0.7299\n",
            "Epoch [9/10], Batch [5500/17426], Loss: 0.7288\n",
            "Epoch [9/10], Batch [5750/17426], Loss: 0.7067\n",
            "Epoch [9/10], Batch [6000/17426], Loss: 0.7160\n",
            "Epoch [9/10], Batch [6250/17426], Loss: 0.7619\n",
            "Epoch [9/10], Batch [6500/17426], Loss: 0.7206\n",
            "Epoch [9/10], Batch [6750/17426], Loss: 0.7459\n",
            "Epoch [9/10], Batch [7000/17426], Loss: 0.6934\n",
            "Epoch [9/10], Batch [7250/17426], Loss: 0.7603\n",
            "Epoch [9/10], Batch [7500/17426], Loss: 0.7428\n",
            "Epoch [9/10], Batch [7750/17426], Loss: 0.7259\n",
            "Epoch [9/10], Batch [8000/17426], Loss: 0.7330\n",
            "Epoch [9/10], Batch [8250/17426], Loss: 0.7006\n",
            "Epoch [9/10], Batch [8500/17426], Loss: 0.7231\n",
            "Epoch [9/10], Batch [8750/17426], Loss: 0.7032\n",
            "Epoch [9/10], Batch [9000/17426], Loss: 0.7384\n",
            "Epoch [9/10], Batch [9250/17426], Loss: 0.6994\n",
            "Epoch [9/10], Batch [9500/17426], Loss: 0.7058\n",
            "Epoch [9/10], Batch [9750/17426], Loss: 0.7332\n",
            "Epoch [9/10], Batch [10000/17426], Loss: 0.7492\n",
            "Epoch [9/10], Batch [10250/17426], Loss: 0.7489\n",
            "Epoch [9/10], Batch [10500/17426], Loss: 0.7399\n",
            "Epoch [9/10], Batch [10750/17426], Loss: 0.7240\n",
            "Epoch [9/10], Batch [11000/17426], Loss: 0.7145\n",
            "Epoch [9/10], Batch [11250/17426], Loss: 0.7290\n",
            "Epoch [9/10], Batch [11500/17426], Loss: 0.7123\n",
            "Epoch [9/10], Batch [11750/17426], Loss: 0.7372\n",
            "Epoch [9/10], Batch [12000/17426], Loss: 0.7271\n",
            "Epoch [9/10], Batch [12250/17426], Loss: 0.7290\n",
            "Epoch [9/10], Batch [12500/17426], Loss: 0.7310\n",
            "Epoch [9/10], Batch [12750/17426], Loss: 0.7351\n",
            "Epoch [9/10], Batch [13000/17426], Loss: 0.7169\n",
            "Epoch [9/10], Batch [13250/17426], Loss: 0.7260\n",
            "Epoch [9/10], Batch [13500/17426], Loss: 0.6925\n",
            "Epoch [9/10], Batch [13750/17426], Loss: 0.7259\n",
            "Epoch [9/10], Batch [14000/17426], Loss: 0.7496\n",
            "Epoch [9/10], Batch [14250/17426], Loss: 0.7220\n",
            "Epoch [9/10], Batch [14500/17426], Loss: 0.7402\n",
            "Epoch [9/10], Batch [14750/17426], Loss: 0.7363\n",
            "Epoch [9/10], Batch [15000/17426], Loss: 0.6990\n",
            "Epoch [9/10], Batch [15250/17426], Loss: 0.7086\n",
            "Epoch [9/10], Batch [15500/17426], Loss: 0.7373\n",
            "Epoch [9/10], Batch [15750/17426], Loss: 0.6999\n",
            "Epoch [9/10], Batch [16000/17426], Loss: 0.7429\n",
            "Epoch [9/10], Batch [16250/17426], Loss: 0.7211\n",
            "Epoch [9/10], Batch [16500/17426], Loss: 0.7315\n",
            "Epoch [9/10], Batch [16750/17426], Loss: 0.7127\n",
            "Epoch [9/10], Batch [17000/17426], Loss: 0.7335\n",
            "Epoch [9/10], Batch [17250/17426], Loss: 0.7169\n",
            "Epoch [9/10] completed with Average Loss: 0.7247\n",
            "Epoch [10/10], Batch [250/17426], Loss: 0.7286\n",
            "Epoch [10/10], Batch [500/17426], Loss: 0.7164\n",
            "Epoch [10/10], Batch [750/17426], Loss: 0.7321\n",
            "Epoch [10/10], Batch [1000/17426], Loss: 0.6952\n",
            "Epoch [10/10], Batch [1250/17426], Loss: 0.7324\n",
            "Epoch [10/10], Batch [1500/17426], Loss: 0.7422\n",
            "Epoch [10/10], Batch [1750/17426], Loss: 0.7279\n",
            "Epoch [10/10], Batch [2000/17426], Loss: 0.7363\n",
            "Epoch [10/10], Batch [2250/17426], Loss: 0.6957\n",
            "Epoch [10/10], Batch [2500/17426], Loss: 0.7051\n",
            "Epoch [10/10], Batch [2750/17426], Loss: 0.7141\n",
            "Epoch [10/10], Batch [3000/17426], Loss: 0.7194\n",
            "Epoch [10/10], Batch [3250/17426], Loss: 0.7162\n",
            "Epoch [10/10], Batch [3500/17426], Loss: 0.7196\n",
            "Epoch [10/10], Batch [3750/17426], Loss: 0.7174\n",
            "Epoch [10/10], Batch [4000/17426], Loss: 0.7184\n",
            "Epoch [10/10], Batch [4250/17426], Loss: 0.7159\n",
            "Epoch [10/10], Batch [4500/17426], Loss: 0.7097\n",
            "Epoch [10/10], Batch [4750/17426], Loss: 0.7080\n",
            "Epoch [10/10], Batch [5000/17426], Loss: 0.7460\n",
            "Epoch [10/10], Batch [5250/17426], Loss: 0.7191\n",
            "Epoch [10/10], Batch [5500/17426], Loss: 0.7280\n",
            "Epoch [10/10], Batch [5750/17426], Loss: 0.7417\n",
            "Epoch [10/10], Batch [6000/17426], Loss: 0.7311\n",
            "Epoch [10/10], Batch [6250/17426], Loss: 0.7175\n",
            "Epoch [10/10], Batch [6500/17426], Loss: 0.7211\n",
            "Epoch [10/10], Batch [6750/17426], Loss: 0.7318\n",
            "Epoch [10/10], Batch [7000/17426], Loss: 0.6886\n",
            "Epoch [10/10], Batch [7250/17426], Loss: 0.7208\n",
            "Epoch [10/10], Batch [7500/17426], Loss: 0.6942\n",
            "Epoch [10/10], Batch [7750/17426], Loss: 0.7224\n",
            "Epoch [10/10], Batch [8000/17426], Loss: 0.7162\n",
            "Epoch [10/10], Batch [8250/17426], Loss: 0.7349\n",
            "Epoch [10/10], Batch [8500/17426], Loss: 0.7716\n",
            "Epoch [10/10], Batch [8750/17426], Loss: 0.6886\n",
            "Epoch [10/10], Batch [9000/17426], Loss: 0.7180\n",
            "Epoch [10/10], Batch [9250/17426], Loss: 0.7121\n",
            "Epoch [10/10], Batch [9500/17426], Loss: 0.7360\n",
            "Epoch [10/10], Batch [9750/17426], Loss: 0.7329\n",
            "Epoch [10/10], Batch [10000/17426], Loss: 0.7454\n",
            "Epoch [10/10], Batch [10250/17426], Loss: 0.7311\n",
            "Epoch [10/10], Batch [10500/17426], Loss: 0.7273\n",
            "Epoch [10/10], Batch [10750/17426], Loss: 0.7235\n",
            "Epoch [10/10], Batch [11000/17426], Loss: 0.7487\n",
            "Epoch [10/10], Batch [11250/17426], Loss: 0.7002\n",
            "Epoch [10/10], Batch [11500/17426], Loss: 0.7126\n",
            "Epoch [10/10], Batch [11750/17426], Loss: 0.7201\n",
            "Epoch [10/10], Batch [12000/17426], Loss: 0.7030\n",
            "Epoch [10/10], Batch [12250/17426], Loss: 0.7094\n",
            "Epoch [10/10], Batch [12500/17426], Loss: 0.6977\n",
            "Epoch [10/10], Batch [12750/17426], Loss: 0.7117\n",
            "Epoch [10/10], Batch [13000/17426], Loss: 0.7369\n",
            "Epoch [10/10], Batch [13250/17426], Loss: 0.7209\n",
            "Epoch [10/10], Batch [13500/17426], Loss: 0.7286\n",
            "Epoch [10/10], Batch [13750/17426], Loss: 0.7100\n",
            "Epoch [10/10], Batch [14000/17426], Loss: 0.7324\n",
            "Epoch [10/10], Batch [14250/17426], Loss: 0.7040\n",
            "Epoch [10/10], Batch [14500/17426], Loss: 0.7074\n",
            "Epoch [10/10], Batch [14750/17426], Loss: 0.7320\n",
            "Epoch [10/10], Batch [15000/17426], Loss: 0.7042\n",
            "Epoch [10/10], Batch [15250/17426], Loss: 0.7165\n",
            "Epoch [10/10], Batch [15500/17426], Loss: 0.7162\n",
            "Epoch [10/10], Batch [15750/17426], Loss: 0.7553\n",
            "Epoch [10/10], Batch [16000/17426], Loss: 0.7045\n",
            "Epoch [10/10], Batch [16250/17426], Loss: 0.7130\n",
            "Epoch [10/10], Batch [16500/17426], Loss: 0.6923\n",
            "Epoch [10/10], Batch [16750/17426], Loss: 0.7642\n",
            "Epoch [10/10], Batch [17000/17426], Loss: 0.7173\n",
            "Epoch [10/10], Batch [17250/17426], Loss: 0.7365\n",
            "Epoch [10/10] completed with Average Loss: 0.7197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now actually Tetsing the model to see output\n",
        "\n",
        "def generate(model, start_string, num_words):\n",
        "  model.eval()\n",
        "  start_string = start_string\n",
        "  input_seq = torch.tensor([letter_to_index[ch] for ch in start_string], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "  hidden = model.init_hidden(1)\n",
        "  hidden = tuple([h.to(device) for h in hidden])\n",
        "\n",
        "  generated = start_string\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i in range(len(start_string) - 1):\n",
        "        _, hidden = model(input_seq[:, i].unsqueeze(1), hidden)\n",
        "\n",
        "    last_char = input_seq[:, -1]\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        output, hidden = model(last_char.unsqueeze(1), hidden)\n",
        "        probabilities = nn.functional.softmax(output.squeeze(), dim=0)\n",
        "        top_char = torch.multinomial(probabilities, 1)[0]\n",
        "        generated += index_to_letter[top_char.item()]\n",
        "        last_char = torch.tensor([top_char], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated\n",
        "\n"
      ],
      "metadata": {
        "id": "WnUHfVg2UmOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this start string how you want\n",
        "start_text = \"How you doing\"\n",
        "generated_text = generate(model, start_text, 500)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lhclpVmViQn",
        "outputId": "d7626f25-3e2f-4cc5-d7af-ad683c58d4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How you doing,\n",
            "To right of her opposited king, our jest,\n",
            "To prove it, and let me do: and art thou hither's use\n",
            "Only Honess commends the sounding:\n",
            "I have forgot, methinks I see many lay\n",
            "A precious boyt with a full rou so? If\n",
            "thy weapons like a servant's drum.\n",
            "Who speak, I do bear my covertors: as marry, Henry, daughter?\n",
            "\n",
            "WARWICK:\n",
            "Is it even now at obe,\n",
            "I would be contented me.\n",
            "\n",
            "GRUMIO:\n",
            "I called my husband's king with his power;\n",
            "Our artied arm Lein to bite this most accursed: underness never\n",
            "Unopempose, the li\n"
          ]
        }
      ]
    }
  ]
}