{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wickedWOLF123/DRP/blob/main/WordModelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Modelling Using LSTM**\n",
        "\n",
        "This is a implementation from https://www.tensorflow.org/text/tutorials/text_generation in TensorFlow based on the http://karpathy.github.io/2015/05/21/rnn-effectiveness/ by Andrej Karapathy. This will be implemented in Pytorch next.\n",
        "\n",
        "We are building a Word Modelling ie next token prediction. We are trying this for generation of Shakespeare like text to understand the usefullness of LSTMS for Natural Language Processing Tasks"
      ],
      "metadata": {
        "id": "qzPVYPprot2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for this\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "wStU3to6pDSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading Shakespeare pieces from googleapis\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Read the data - this file has little over a  million characters and\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'text length: {len(text)}')\n",
        "\n",
        "vocabulary = sorted(set(text))\n",
        "print(f'Unique Characters: {len(vocabulary)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufyFol7ApvUm",
        "outputId": "6740442a-a12c-4012-a1eb-9c10e7440e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "text length: 1115394\n",
            "Unique Characters: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement a similar character-based text generation model using PyTorch, you can follow the steps below. We’ll use the PyTorch framework to recreate the same RNN model architecture and training flow demonstrated with TensorFlow.\n",
        "\n",
        "Steps to Recreate RNN Text Generation in PyTorch\n",
        "1. Install Dependencies:\n",
        "\n",
        "  bash\n",
        "  Copy code\n",
        "  pip install torch torchvision\n",
        "2. Import Libraries:\n",
        "\n",
        "  python\n",
        "  Copy code\n",
        "  import torch\n",
        "  import torch.nn as nn\n",
        "  import numpy as np\n",
        "  import os\n",
        "  import time\n",
        "3. Download the Shakespeare Dataset:\n",
        "  python\n",
        "  Copy code\n",
        "  # Download the dataset\n",
        "  import requests\n",
        "\n",
        "  url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "  response = requests.get(url)\n",
        "  text = response.text\n",
        "\n",
        "  print(f'Length of text: {len(text)} characters')\n",
        "  print(text[:250])  # View a sample of the text\n",
        "\n",
        "# Get the unique characters (vocabulary)\n",
        "  vocab = sorted(set(text))\n",
        "  vocab_size = len(vocab)\n",
        "  print(f'{vocab_size} unique characters')\n",
        "4. Create Character Mapping:\n",
        "python\n",
        "Copy code\n",
        "# Create character to index and index to character mappings\n",
        "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# Convert text into numerical data (IDs)\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Function to convert IDs back to text\n",
        "def text_from_ids(ids):\n",
        "    return ''.join(idx2char[ids])\n",
        "5. Create Input and Target Sequences:\n",
        "python\n",
        "Copy code\n",
        "# Sequence length for input and target\n",
        "seq_length = 100  # Each input sequence will have 100 characters\n",
        "examples_per_epoch = len(text) // seq_length\n",
        "\n",
        "# Split the text into input-target pairs\n",
        "def create_sequences(data, seq_length):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(0, len(data) - seq_length):\n",
        "        inputs.append(data[i:i + seq_length])\n",
        "        targets.append(data[i + 1:i + 1 + seq_length])\n",
        "    return np.array(inputs), np.array(targets)\n",
        "\n",
        "inputs, targets = create_sequences(text_as_int, seq_length)\n",
        "inputs = torch.tensor(inputs, dtype=torch.long)\n",
        "targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "print(f'Input size: {inputs.size()}, Target size: {targets.size()}')\n",
        "6. Define the RNN Model:\n",
        "python\n",
        "Copy code\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.GRU(embedding_dim, rnn_units, batch_first=True)\n",
        "        self.fc = nn.Linear(rnn_units, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "7. Set Hyperparameters:\n",
        "python\n",
        "Copy code\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "model = CharRNN(vocab_size, embedding_dim, rnn_units).to('cuda')\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "8. Create DataLoader for Batching:\n",
        "python\n",
        "Copy code\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "dataset = TensorDataset(inputs, targets)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "9. Train the Model:\n",
        "python\n",
        "Copy code\n",
        "def train_model(model, dataloader, loss_fn, optimizer, epochs):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "        total_loss = 0\n",
        "        hidden = None  # Reset hidden state at each epoch\n",
        "\n",
        "        for batch, (inp, target) in enumerate(dataloader):\n",
        "            inp, target = inp.cuda(), target.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            output, hidden = model(inp, hidden)\n",
        "            hidden = hidden.detach()  # Detach hidden state to avoid backprop through entire history\n",
        "\n",
        "            # Reshape output and target for loss calculation\n",
        "            loss = loss_fn(output.view(-1, vocab_size), target.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Backprop and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch % 50 == 0:\n",
        "                print(f'Epoch {epoch + 1}, Batch {batch}, Loss: {loss.item()}')\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}')\n",
        "        print(f'Time for epoch: {time.time() - start:.2f} sec')\n",
        "\n",
        "# Train the model\n",
        "train_model(model, dataloader, loss_fn, optimizer, epochs=20)\n",
        "10. Generate Text Using the Trained Model:\n",
        "python\n",
        "Copy code\n",
        "def generate_text(model, start_string, num_generate=1000, temperature=1.0):\n",
        "    model.eval()\n",
        "\n",
        "    input_eval = torch.tensor([char2idx[c] for c in start_string], dtype=torch.long).unsqueeze(0).to('cuda')\n",
        "    hidden = None\n",
        "    generated_text = start_string\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        output, hidden = model(input_eval, hidden)\n",
        "        output = output / temperature  # Adjust randomness with temperature\n",
        "        predicted_id = torch.multinomial(torch.softmax(output[0, -1], dim=0), 1).item()\n",
        "\n",
        "        # Add the predicted character to the generated text\n",
        "        generated_text += idx2char[predicted_id]\n",
        "\n",
        "        # Feed the predicted character as input for the next step\n",
        "        input_eval = torch.tensor([[predicted_id]], dtype=torch.long).to('cuda')\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate text\n",
        "print(generate_text(model, start_string=\"ROMEO: \"))\n",
        "11. Save and Load the Model:\n",
        "python\n",
        "Copy code\n",
        "# Save model weights\n",
        "torch.save(model.state_dict(), 'char_rnn.pth')\n",
        "\n",
        "# Load model weights\n",
        "model = CharRNN(vocab_size, embedding_dim, rnn_units).to('cuda')\n",
        "model.load_state_dict(torch.load('char_rnn.pth'))\n",
        "Explanation:\n",
        "Model Architecture: We use an embedding layer to convert character indices into dense vectors, followed by a GRU (RNN) layer and a dense output layer to predict the next character.\n",
        "Training: The model is trained using sequences of 100 characters. Each input is a sequence, and the target is the same sequence shifted by one character.\n",
        "Text Generation: During generation, the model takes the last predicted character as input for the next step, sampling characters based on probabilities.\n",
        "This PyTorch implementation mirrors the TensorFlow code but leverages PyTorch’s RNN framework, including manual handling of hidden states for sequential text generation. Adjust the model hyperparameters and the number of training epochs for better results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gDdZ6MpNhaH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we create a unique id for each char as we can directly pass characters to the\n",
        "# model we pass vectorized ids to the model\n",
        "\n",
        "# We also need to do the inverse as we want to generate text and to do that we need to be\n",
        "# able to convert the output vectors back to the original characters\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocabulary), mask_token=None)\n",
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "# Function to join all text back together\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n"
      ],
      "metadata": {
        "id": "lwPj4b8YqXCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will be dividing the input sequence : text into smaller chunks\n",
        "# Our input will be the num_of_chars and the output should be of the same length but stating from the second character\n",
        "# Eg - Hello is our target, seq_length = 4, input = \"Hello\", output = \"ello\"\n",
        "\n",
        "seq_length = 100\n",
        "\n",
        "# Get ids of all characters\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "\n",
        "# Slice this encoded list\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n"
      ],
      "metadata": {
        "id": "zGPcwDgSsjry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As we are always predicting next character our dataset has (input, label) pair as\n",
        "# current_char, next_char\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# this is an example with seq_length character sequences\n",
        "# for input_example, target_example in dataset.take(1):\n",
        "#     print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "#     print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "id": "Oqh3CF_mAcSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will be packing dataset into batches fro effeciency\n",
        "# and also shuffle the data\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "metadata": {
        "id": "3TYMx0zWBJVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the Model**\n",
        "\n",
        "Although the implementation uses a GRU we will try implementing with both a GRU and an LSTM. Our implementation for 1 time series is as follows:\n",
        "\n",
        "1. **Embedding:** embed the id of the input character to a vector with embed num of dims. Doing this with a one hot vector leads to just the column of our weight matrix\n",
        "\n",
        "2. **RNN:** The LSTM/GRU layer\n",
        "\n",
        "3.**Dense layer: ** This is essentially a fully connected layer that shows log likelyhood of each character"
      ],
      "metadata": {
        "id": "ETL6I7MIB4Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize some parameters\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "R8JxBHJFMJbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            reset_after=True,\n",
        "        )\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KE0akPrOEAJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n"
      ],
      "metadata": {
        "id": "1MxKxShipQTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions, states = model(input_example_batch, return_state=True)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ],
      "metadata": {
        "id": "ntuLAk0XMftO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "28815f52-4749-4f56-fe49-9e1760bcbf06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling MyModel.call().\n\n\u001b[1mtoo many values to unpack (expected 2)\u001b[0m\n\nArguments received by MyModel.call():\n  • inputs=tf.Tensor(shape=(64, 100), dtype=int64)\n  • states=None\n  • return_state=True\n  • training=None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-99c7669d0100>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_example_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_example_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mexample_batch_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_example_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_batch_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"# (batch_size, sequence_length, vocab_size)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-999b1aad38d5>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, states, return_state, training)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling MyModel.call().\n\n\u001b[1mtoo many values to unpack (expected 2)\u001b[0m\n\nArguments received by MyModel.call():\n  • inputs=tf.Tensor(shape=(64, 100), dtype=int64)\n  • states=None\n  • return_state=True\n  • training=None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw_tpUMYOT8-",
        "outputId": "4ea18099-325d-48f3-beca-1024e08fb10e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.1874623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix + '.weights.h5',\n",
        "    save_weights_only=True)\n"
      ],
      "metadata": {
        "id": "kxvNijkLOmfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OmwAmGB-hW30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=10\n",
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model = model(vocab_size, embedding_dim, rnn_units)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "K2ZVZYuyOuMe",
        "outputId": "81bc47f0-e79c-4ba0-c510-7aefeb8134be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: 66 (of type <class 'int'>)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-f5f074a1726f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 ):\n\u001b[0;32m--> 789\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    790\u001b[0m                         \u001b[0;34m\"Only input tensors may be passed as \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                         \u001b[0;34m\"positional arguments. The following argument value \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: 66 (of type <class 'int'>)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TYcwEC2AhZ2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uF5Q4O0zhZ55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J1Eg5exQhaAH"
      }
    }
  ]
}