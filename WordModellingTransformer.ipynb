{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wickedWOLF123/DRP/blob/main/WordModellingTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pudm7ttDjAR"
      },
      "outputs": [],
      "source": [
        "#Imports for this project\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38C5iPS7AUKF",
        "outputId": "2e4c2788-1389-4757-cbd5-f8ab436d6d18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# Getting Shakespear writing as text file from googleapis\n",
        "import requests\n",
        "\n",
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "print(text[:250])\n",
        "\n",
        "# We need to convert every character to a vector so were see how many unique characters\n",
        "# These unique characters make up our vocabulary\n",
        "vocabulary = sorted(set(text))\n",
        "vocab_size = len(vocabulary)\n",
        "print(f'{len(vocabulary)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91vQ-1V4AVRV"
      },
      "outputs": [],
      "source": [
        "# Create mappings from characters to vectors and vice-versa\n",
        "letter_to_index = {character: idx for idx, character in enumerate(vocabulary)}\n",
        "index_to_letter = {idx: character for idx, character in enumerate(vocabulary)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4kpxBKs7Xz8"
      },
      "source": [
        "It might be worth randomly choosing the lengths of the character input to be between say 10-100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozr75cwlAXJ7"
      },
      "outputs": [],
      "source": [
        "# Were going to chop up our 1000000+ character input into 100 size pieces\n",
        "# So that it is easier and we get batch processing\n",
        "sequence_length = 100\n",
        "encoded_text = np.array([letter_to_index[ch] for ch in text], dtype=np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRMVAHin42pG",
        "outputId": "827d78f4-3074-485e-e9f6-c39d9ea0142a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(encoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUTNZEzL9RCu"
      },
      "source": [
        "To have sequences of random length, but on average, of length 50, I recommend doing something like the following. One of the caveats is that the output might sometimes be the input of something they've seen before, but then again this is gonna be stuff they are seeing during the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I7GXYT88p5W",
        "outputId": "a4c8a9ec-285d-4e7b-ad75-46a98fbc4806"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([     38,      51,     119, ..., 1115291, 1115354, 1115385])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "# generate the indices associated with the encoded texts\n",
        "idxs = [i for i in range(len(encoded_text))]\n",
        "# randomly pick indices and sort them. There on average,\n",
        "#there are 1/50th of the indices chosen, so they should differ on average, length 50\n",
        "np.sort(random.sample(idxs, int(len(idxs)/50)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAxN7usT5iPS"
      },
      "source": [
        "One issue with constructing the dataset in this format is that it results in the dataset being around 16 times larger than it was before, and this might require a larger dataset than we have ram. Tested it out, and getting from a sequence length of size 50 to 90 requires around 80gb of ram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcXXVKxzAXMo",
        "outputId": "f3062928-81b3-4d27-b1bf-b7eeffb3074e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequence Length: 50\n"
          ]
        }
      ],
      "source": [
        "# Slice the encoded into sizes of encoded lenght\n",
        "# Our input sequence is from [0: seq_len] and the\n",
        "# the target sequence is [1: seq_len+1], now we loop\n",
        "input_sequences = []\n",
        "output_sequences = []\n",
        "\n",
        "# Adjusting sequence generation based on incremental lengths\n",
        "for seq in range(50,51):\n",
        "    print(f'Sequence Length: {seq}')\n",
        "    for i in range(len(encoded_text) - seq):\n",
        "        input_sequences.append(list(encoded_text[i:i+seq]))\n",
        "        output_sequences.append(list(encoded_text[i+1:i+seq+1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1snsMT1zuygm",
        "outputId": "683bce5e-99fd-47dc-aa58-35251bdb89d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequences = 1115344\n"
          ]
        }
      ],
      "source": [
        "max_len = sequence_length\n",
        "\n",
        "# Pad sequences to max_len\n",
        "input_sequences_padded = [seq + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in input_sequences]\n",
        "output_sequences_padded = [seq + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in output_sequences]\n",
        "\n",
        "# Convert lists to numpy arrays for faster processing\n",
        "inputs_array = np.array(input_sequences_padded, dtype=np.int64)\n",
        "outputs_array = np.array(output_sequences_padded, dtype=np.int64)\n",
        "\n",
        "# Now convert numpy arrays to tensors\n",
        "inputs_tensor = torch.tensor(inputs_array, dtype=torch.long)\n",
        "outputs_tensor = torch.tensor(outputs_array, dtype=torch.long)\n",
        "\n",
        "print(f'Sequences = {len(input_sequences)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msYqkXWubK8f"
      },
      "outputs": [],
      "source": [
        "# PARAMETERS\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 10\n",
        "EMBED_SIZE = 512\n",
        "HIDDEN_SIZE = 2048\n",
        "NUM_LAYERS = 6\n",
        "NUM_HEADS = 8\n",
        "DROPOUT = 0.1\n",
        "LEARNING_RATE = 0.001\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQqOQJm9Ajni",
        "outputId": "31f7c610-d71f-48f3-e1cc-74265cb5ba74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-3d4a72b7ea97>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.input = torch.tensor(input, dtype=torch.long)\n",
            "<ipython-input-10-3d4a72b7ea97>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.output = torch.tensor(output, dtype=torch.long)\n"
          ]
        }
      ],
      "source": [
        "# Creating the Dataset in Pytorch and change them to tensors\n",
        "\n",
        "class shakespeareDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, input, output):\n",
        "    self.input = torch.tensor(input, dtype=torch.long)\n",
        "    self.output = torch.tensor(output, dtype=torch.long)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input[idx], self.output[idx]\n",
        "\n",
        "dataset = shakespeareDataset(inputs_tensor, outputs_tensor)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE , shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VMFPBB6AoS_"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, num_heads, sequence_length, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        # Token embedding\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = self.create_positional_encoding(sequence_length, embed_size)\n",
        "        # Transformer encoder layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def create_positional_encoding(self, sequence_length, embed_size):\n",
        "        pe = torch.zeros(sequence_length, embed_size)\n",
        "        position = torch.arange(0, sequence_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-np.log(10000.0) / embed_size))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        if embed_size % 2 == 1:\n",
        "            # If embed_size is odd, adjust the size of div_term\n",
        "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
        "        else:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size()\n",
        "        positional_encoding = self.positional_encoding[:, :seq_len, :].to(x.device)\n",
        "        x = self.token_embedding(x) * np.sqrt(self.embed_size)\n",
        "        x = x + positional_encoding\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.fc_out(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W-Vus9_BPh2"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "NUM_HEADS = 8\n",
        "DROPOUT = 0.1\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Instantiate the model\n",
        "model = TransformerModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_size=EMBED_SIZE,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    sequence_length=sequence_length,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6Hj0y92kBfuf",
        "outputId": "98e5b140-f3dc-4355-dd57-37309d538286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/10, Batch: 100/4356, Loss: 1.3820, Time/Batch: 0.22s\n",
            "Epoch: 1/10, Batch: 200/4356, Loss: 1.1578, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 300/4356, Loss: 1.0723, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 400/4356, Loss: 1.0370, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 500/4356, Loss: 1.0212, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 600/4356, Loss: 1.0123, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 700/4356, Loss: 1.0037, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 800/4356, Loss: 0.9970, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 900/4356, Loss: 0.9916, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 1000/4356, Loss: 0.9861, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 1100/4356, Loss: 0.9810, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 1200/4356, Loss: 0.9793, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 1300/4356, Loss: 0.9764, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 1400/4356, Loss: 0.9727, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 1500/4356, Loss: 0.9712, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 1600/4356, Loss: 0.9694, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 1700/4356, Loss: 0.9652, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 1800/4356, Loss: 0.9646, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 1900/4356, Loss: 0.9619, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 2000/4356, Loss: 0.9607, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 2100/4356, Loss: 0.9597, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 2200/4356, Loss: 0.9578, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 2300/4356, Loss: 0.9570, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 2400/4356, Loss: 0.9544, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 2500/4356, Loss: 0.9545, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 2600/4356, Loss: 0.9530, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 2700/4356, Loss: 0.9494, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 2800/4356, Loss: 0.9505, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 2900/4356, Loss: 0.9501, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 3000/4356, Loss: 0.9470, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 3100/4356, Loss: 0.9477, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 3200/4356, Loss: 0.9459, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 3300/4356, Loss: 0.9454, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 3400/4356, Loss: 0.9431, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 3500/4356, Loss: 0.9412, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 3600/4356, Loss: 0.9418, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 3700/4356, Loss: 0.9403, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 3800/4356, Loss: 0.9389, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 3900/4356, Loss: 0.9390, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 4000/4356, Loss: 0.9388, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 4100/4356, Loss: 0.9361, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 4200/4356, Loss: 0.9371, Time/Batch: 0.21s\n",
            "Epoch: 1/10, Batch: 4300/4356, Loss: 0.9346, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 100/4356, Loss: 0.9418, Time/Batch: 0.22s\n",
            "Epoch: 2/10, Batch: 200/4356, Loss: 0.9316, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 300/4356, Loss: 0.9311, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 400/4356, Loss: 0.9300, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 500/4356, Loss: 0.9307, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 600/4356, Loss: 0.9280, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 700/4356, Loss: 0.9265, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 800/4356, Loss: 0.9263, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 900/4356, Loss: 0.9253, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 1000/4356, Loss: 0.9244, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 1100/4356, Loss: 0.9235, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 1200/4356, Loss: 0.9240, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 1300/4356, Loss: 0.9233, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 1400/4356, Loss: 0.9197, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 1500/4356, Loss: 0.9018, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 1600/4356, Loss: 0.8414, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 1700/4356, Loss: 0.7893, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 1800/4356, Loss: 0.7464, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 1900/4356, Loss: 0.7127, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 2000/4356, Loss: 0.6861, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 2100/4356, Loss: 0.6633, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 2200/4356, Loss: 0.6379, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 2300/4356, Loss: 0.6145, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 2400/4356, Loss: 0.5852, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 2500/4356, Loss: 0.5056, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 2600/4356, Loss: 0.4279, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 2700/4356, Loss: 0.3764, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 2800/4356, Loss: 0.3428, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 2900/4356, Loss: 0.3285, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 3000/4356, Loss: 0.3087, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 3100/4356, Loss: 0.2884, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 3200/4356, Loss: 0.2673, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 3300/4356, Loss: 0.2517, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 3400/4356, Loss: 0.2320, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 3500/4356, Loss: 0.2212, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 3600/4356, Loss: 0.2100, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 3700/4356, Loss: 0.1932, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 3800/4356, Loss: 0.1834, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 3900/4356, Loss: 0.1732, Time/Batch: 0.21s\n",
            "Epoch: 2/10, Batch: 4000/4356, Loss: 0.1621, Time/Batch: 0.21s\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "model.train(mode=True)\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch, (inp, target) in enumerate(dataloader):\n",
        "        inp = inp.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(inp)\n",
        "\n",
        "        # Reshape output and target for computing loss\n",
        "        output = output.view(-1, vocab_size)\n",
        "        target = target.reshape(-1)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping (optional but helps with training stability)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % 100 == 0 and batch > 0:\n",
        "            avg_loss = total_loss / 100\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f'Epoch: {epoch+1}/{NUM_EPOCHS}, Batch: {batch}/{len(dataloader)}, Loss: {avg_loss:.4f}, Time/Batch: {elapsed/100:.2f}s')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lR__xD4BlLR"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_text, generate_length=100):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    generated_text = start_text\n",
        "    input_indices = [letter_to_index.get(c, 0) for c in start_text]\n",
        "    input_tensor = torch.tensor([input_indices], dtype=torch.long).to(device)\n",
        "\n",
        "    for _ in range(generate_length):\n",
        "        # Ensure input tensor is of shape (batch_size=1, sequence_length)\n",
        "        input_seq = input_tensor[:, -sequence_length:]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_seq)\n",
        "\n",
        "        # Get the logits for the last time step\n",
        "        logits = output[:, -1, :]  # Shape: (1, vocab_size)\n",
        "        probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Sample from the distribution or take the most probable token\n",
        "        next_token = torch.multinomial(probabilities, num_samples=1).item()\n",
        "\n",
        "        # Append generated character\n",
        "        generated_text += index_to_letter[next_token]\n",
        "\n",
        "        # Update input tensor\n",
        "        input_tensor = torch.cat([input_tensor, torch.tensor([[next_token]], dtype=torch.long).to(device)], dim=1)\n",
        "\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a35f6e_BpIB"
      },
      "outputs": [],
      "source": [
        "start_text = \"Romeo: \"\n",
        "generated_text = generate_text(model, start_text, generate_length=200)\n",
        "print(generated_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}